{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential # Class\n",
    "from keras.layers import Dense # Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "[ 2  4  6  8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40]\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,15,16,17,18,19,20])\n",
    "y_train = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40])\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.sequential.Sequential'>\n",
      "Train on 16 samples, validate on 4 samples\n",
      "Epoch 1/300\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 938.1318 - acc: 0.0000e+00 - val_loss: 3429.7806 - val_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 929.5553 - acc: 0.0000e+00 - val_loss: 3403.2958 - val_acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 921.9942 - acc: 0.0000e+00 - val_loss: 3375.3775 - val_acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 914.0855 - acc: 0.0000e+00 - val_loss: 3348.2474 - val_acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 906.4470 - acc: 0.0000e+00 - val_loss: 3320.0587 - val_acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 898.5223 - acc: 0.0000e+00 - val_loss: 3293.0300 - val_acc: 0.0000e+00\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 890.7445 - acc: 0.0000e+00 - val_loss: 3266.5051 - val_acc: 0.0000e+00\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 883.2973 - acc: 0.0000e+00 - val_loss: 3238.9431 - val_acc: 0.0000e+00\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 875.2115 - acc: 0.0000e+00 - val_loss: 3214.8304 - val_acc: 0.0000e+00\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 868.1563 - acc: 0.0000e+00 - val_loss: 3187.0545 - val_acc: 0.0000e+00\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 860.7999 - acc: 0.0000e+00 - val_loss: 3158.8490 - val_acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 853.1886 - acc: 0.0000e+00 - val_loss: 3131.8730 - val_acc: 0.0000e+00\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 845.6373 - acc: 0.0000e+00 - val_loss: 3106.1214 - val_acc: 0.0000e+00\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 838.2943 - acc: 0.0000e+00 - val_loss: 3080.6754 - val_acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 830.8136 - acc: 0.0000e+00 - val_loss: 3056.7466 - val_acc: 0.0000e+00\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 823.7467 - acc: 0.0000e+00 - val_loss: 3031.2762 - val_acc: 0.0000e+00\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 816.5297 - acc: 0.0000e+00 - val_loss: 3005.9615 - val_acc: 0.0000e+00\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 809.6017 - acc: 0.0000e+00 - val_loss: 2979.4864 - val_acc: 0.0000e+00\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 802.5421 - acc: 0.0000e+00 - val_loss: 2952.9453 - val_acc: 0.0000e+00\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 795.4297 - acc: 0.0000e+00 - val_loss: 2927.4796 - val_acc: 0.0000e+00\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 788.0646 - acc: 0.0000e+00 - val_loss: 2903.9874 - val_acc: 0.0000e+00\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 781.2270 - acc: 0.0000e+00 - val_loss: 2880.2014 - val_acc: 0.0000e+00\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 774.3297 - acc: 0.0000e+00 - val_loss: 2856.1716 - val_acc: 0.0000e+00\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 767.8080 - acc: 0.0000e+00 - val_loss: 2830.2271 - val_acc: 0.0000e+00\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 760.7661 - acc: 0.0000e+00 - val_loss: 2805.8151 - val_acc: 0.0000e+00\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 753.7664 - acc: 0.0000e+00 - val_loss: 2782.8171 - val_acc: 0.0000e+00\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 747.5655 - acc: 0.0000e+00 - val_loss: 2757.2393 - val_acc: 0.0000e+00\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 740.3013 - acc: 0.0000e+00 - val_loss: 2734.9311 - val_acc: 0.0000e+00\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 733.6103 - acc: 0.0000e+00 - val_loss: 2713.5046 - val_acc: 0.0000e+00\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 727.4900 - acc: 0.0000e+00 - val_loss: 2688.8395 - val_acc: 0.0000e+00\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 720.8624 - acc: 0.0000e+00 - val_loss: 2665.2907 - val_acc: 0.0000e+00\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 714.3987 - acc: 0.0000e+00 - val_loss: 2641.2999 - val_acc: 0.0000e+00\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 707.7592 - acc: 0.0000e+00 - val_loss: 2618.3713 - val_acc: 0.0000e+00\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 701.2013 - acc: 0.0000e+00 - val_loss: 2596.5709 - val_acc: 0.0000e+00\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - ETA: 0s - loss: 1929.5255 - acc: 0.0000e+ - 0s 1ms/step - loss: 694.9613 - acc: 0.0000e+00 - val_loss: 2573.9406 - val_acc: 0.0000e+00\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 688.4774 - acc: 0.0000e+00 - val_loss: 2552.4065 - val_acc: 0.0000e+00\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 682.3078 - acc: 0.0000e+00 - val_loss: 2530.1432 - val_acc: 0.0000e+00\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 676.2749 - acc: 0.0000e+00 - val_loss: 2506.7105 - val_acc: 0.0000e+00\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 669.8397 - acc: 0.0000e+00 - val_loss: 2484.6937 - val_acc: 0.0000e+00\n",
      "Epoch 40/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 663.7545 - acc: 0.0000e+00 - val_loss: 2462.4600 - val_acc: 0.0000e+00\n",
      "Epoch 41/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 657.7642 - acc: 0.0000e+00 - val_loss: 2439.6752 - val_acc: 0.0000e+00\n",
      "Epoch 42/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 651.3674 - acc: 0.0000e+00 - val_loss: 2418.6761 - val_acc: 0.0000e+00\n",
      "Epoch 43/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 645.1342 - acc: 0.0000e+00 - val_loss: 2399.0952 - val_acc: 0.0000e+00\n",
      "Epoch 44/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 639.5330 - acc: 0.0000e+00 - val_loss: 2377.2844 - val_acc: 0.0000e+00\n",
      "Epoch 45/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 633.3336 - acc: 0.0000e+00 - val_loss: 2356.9504 - val_acc: 0.0000e+00\n",
      "Epoch 46/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 627.3998 - acc: 0.0000e+00 - val_loss: 2336.8294 - val_acc: 0.0000e+00\n",
      "Epoch 47/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 621.9471 - acc: 0.0000e+00 - val_loss: 2313.8566 - val_acc: 0.0000e+00\n",
      "Epoch 48/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 615.6728 - acc: 0.0000e+00 - val_loss: 2293.7417 - val_acc: 0.0000e+00\n",
      "Epoch 49/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 610.2909 - acc: 0.0000e+00 - val_loss: 2271.3509 - val_acc: 0.0000e+00\n",
      "Epoch 50/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 604.3889 - acc: 0.0000e+00 - val_loss: 2250.0012 - val_acc: 0.0000e+00\n",
      "Epoch 51/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 598.3118 - acc: 0.0000e+00 - val_loss: 2230.6998 - val_acc: 0.0000e+00\n",
      "Epoch 52/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 593.1199 - acc: 0.0000e+00 - val_loss: 2209.1367 - val_acc: 0.0000e+00\n",
      "Epoch 53/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 587.1503 - acc: 0.0000e+00 - val_loss: 2189.2223 - val_acc: 0.0000e+00\n",
      "Epoch 54/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 581.1254 - acc: 0.0000e+00 - val_loss: 2172.2046 - val_acc: 0.0000e+00\n",
      "Epoch 55/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 576.1222 - acc: 0.0000e+00 - val_loss: 2151.7460 - val_acc: 0.0000e+00\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 570.5561 - acc: 0.0000e+00 - val_loss: 2131.6133 - val_acc: 0.0000e+00\n",
      "Epoch 57/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 565.0628 - acc: 0.0000e+00 - val_loss: 2111.5625 - val_acc: 0.0000e+00\n",
      "Epoch 58/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 559.6437 - acc: 0.0000e+00 - val_loss: 2091.5260 - val_acc: 0.0000e+00\n",
      "Epoch 59/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 553.7903 - acc: 0.0000e+00 - val_loss: 2074.4198 - val_acc: 0.0000e+00\n",
      "Epoch 60/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 549.1713 - acc: 0.0000e+00 - val_loss: 2053.1053 - val_acc: 0.0000e+00\n",
      "Epoch 61/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 543.3811 - acc: 0.0000e+00 - val_loss: 2034.1438 - val_acc: 0.0000e+00\n",
      "Epoch 62/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 537.9057 - acc: 0.0000e+00 - val_loss: 2016.4144 - val_acc: 0.0000e+00\n",
      "Epoch 63/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 533.1727 - acc: 0.0000e+00 - val_loss: 1995.9255 - val_acc: 0.0000e+00\n",
      "Epoch 64/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 527.4526 - acc: 0.0000e+00 - val_loss: 1978.1240 - val_acc: 0.0000e+00\n",
      "Epoch 65/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 522.3284 - acc: 0.0000e+00 - val_loss: 1960.1609 - val_acc: 0.0000e+00\n",
      "Epoch 66/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 517.3603 - acc: 0.0000e+00 - val_loss: 1941.2212 - val_acc: 0.0000e+00\n",
      "Epoch 67/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 511.8928 - acc: 0.0000e+00 - val_loss: 1924.8299 - val_acc: 0.0000e+00\n",
      "Epoch 68/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 507.4220 - acc: 0.0000e+00 - val_loss: 1904.6678 - val_acc: 0.0000e+00\n",
      "Epoch 69/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 501.7697 - acc: 0.0000e+00 - val_loss: 1888.2372 - val_acc: 0.0000e+00\n",
      "Epoch 70/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 497.2773 - acc: 0.0000e+00 - val_loss: 1868.7395 - val_acc: 0.0000e+00\n",
      "Epoch 71/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 492.1464 - acc: 0.0000e+00 - val_loss: 1850.3469 - val_acc: 0.0000e+00\n",
      "Epoch 72/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 487.0305 - acc: 0.0000e+00 - val_loss: 1832.9273 - val_acc: 0.0000e+00\n",
      "Epoch 73/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 481.9607 - acc: 0.0000e+00 - val_loss: 1816.7781 - val_acc: 0.0000e+00\n",
      "Epoch 74/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 477.3213 - acc: 0.0000e+00 - val_loss: 1799.3300 - val_acc: 0.0000e+00\n",
      "Epoch 75/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 472.4171 - acc: 0.0000e+00 - val_loss: 1782.3117 - val_acc: 0.0000e+00\n",
      "Epoch 76/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 467.8276 - acc: 0.0000e+00 - val_loss: 1764.2206 - val_acc: 0.0000e+00\n",
      "Epoch 77/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 463.1631 - acc: 0.0000e+00 - val_loss: 1746.0543 - val_acc: 0.0000e+00\n",
      "Epoch 78/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 458.0247 - acc: 0.0000e+00 - val_loss: 1730.0352 - val_acc: 0.0000e+00\n",
      "Epoch 79/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 453.4173 - acc: 0.0000e+00 - val_loss: 1713.6486 - val_acc: 0.0000e+00\n",
      "Epoch 80/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 448.8602 - acc: 0.0000e+00 - val_loss: 1696.7009 - val_acc: 0.0000e+00\n",
      "Epoch 81/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 444.1043 - acc: 0.0000e+00 - val_loss: 1680.7932 - val_acc: 0.0000e+00\n",
      "Epoch 82/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 439.5356 - acc: 0.0000e+00 - val_loss: 1664.6843 - val_acc: 0.0000e+00\n",
      "Epoch 83/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 435.2617 - acc: 0.0000e+00 - val_loss: 1647.1034 - val_acc: 0.0000e+00\n",
      "Epoch 84/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 430.3500 - acc: 0.0000e+00 - val_loss: 1631.9101 - val_acc: 0.0000e+00\n",
      "Epoch 85/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 426.0887 - acc: 0.0000e+00 - val_loss: 1615.4126 - val_acc: 0.0000e+00\n",
      "Epoch 86/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 421.7496 - acc: 0.0000e+00 - val_loss: 1598.4778 - val_acc: 0.0000e+00\n",
      "Epoch 87/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 416.9209 - acc: 0.0000e+00 - val_loss: 1584.3608 - val_acc: 0.0000e+00\n",
      "Epoch 88/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 412.9250 - acc: 0.0000e+00 - val_loss: 1567.4708 - val_acc: 0.0000e+00\n",
      "Epoch 89/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 408.2293 - acc: 0.0000e+00 - val_loss: 1553.0566 - val_acc: 0.0000e+00\n",
      "Epoch 90/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 404.1927 - acc: 0.0000e+00 - val_loss: 1536.7773 - val_acc: 0.0000e+00\n",
      "Epoch 91/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 399.9603 - acc: 0.0000e+00 - val_loss: 1520.2117 - val_acc: 0.0000e+00\n",
      "Epoch 92/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 395.5331 - acc: 0.0000e+00 - val_loss: 1504.7050 - val_acc: 0.0000e+00\n",
      "Epoch 93/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 390.9909 - acc: 0.0000e+00 - val_loss: 1491.2941 - val_acc: 0.0000e+00\n",
      "Epoch 94/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 387.1696 - acc: 0.0000e+00 - val_loss: 1475.4425 - val_acc: 0.0000e+00\n",
      "Epoch 95/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 383.1734 - acc: 0.0000e+00 - val_loss: 1459.0224 - val_acc: 0.0000e+00\n",
      "Epoch 96/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 378.8206 - acc: 0.0000e+00 - val_loss: 1443.9042 - val_acc: 0.0000e+00\n",
      "Epoch 97/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 374.7399 - acc: 0.0000e+00 - val_loss: 1428.8109 - val_acc: 0.0000e+00\n",
      "Epoch 98/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 370.4459 - acc: 0.0000e+00 - val_loss: 1414.8719 - val_acc: 0.0000e+00\n",
      "Epoch 99/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 366.7902 - acc: 0.0000e+00 - val_loss: 1399.3458 - val_acc: 0.0000e+00\n",
      "Epoch 100/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 362.5979 - acc: 0.0000e+00 - val_loss: 1384.8069 - val_acc: 0.0000e+00\n",
      "Epoch 101/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 358.3291 - acc: 0.0000e+00 - val_loss: 1371.8431 - val_acc: 0.0000e+00\n",
      "Epoch 102/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 354.5095 - acc: 0.0000e+00 - val_loss: 1358.3316 - val_acc: 0.0000e+00\n",
      "Epoch 103/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 350.6078 - acc: 0.0000e+00 - val_loss: 1344.7942 - val_acc: 0.0000e+00\n",
      "Epoch 104/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 346.9137 - acc: 0.0000e+00 - val_loss: 1330.1523 - val_acc: 0.0000e+00\n",
      "Epoch 105/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 342.9865 - acc: 0.0000e+00 - val_loss: 1316.1795 - val_acc: 0.0000e+00\n",
      "Epoch 106/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 339.2063 - acc: 0.0000e+00 - val_loss: 1301.9537 - val_acc: 0.0000e+00\n",
      "Epoch 107/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 335.3452 - acc: 0.0000e+00 - val_loss: 1288.2508 - val_acc: 0.0000e+00\n",
      "Epoch 108/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 331.6744 - acc: 0.0000e+00 - val_loss: 1274.1811 - val_acc: 0.0000e+00\n",
      "Epoch 109/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 327.7993 - acc: 0.0000e+00 - val_loss: 1260.9124 - val_acc: 0.0000e+00\n",
      "Epoch 110/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 324.0134 - acc: 0.0000e+00 - val_loss: 1248.1693 - val_acc: 0.0000e+00\n",
      "Epoch 111/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 320.7472 - acc: 0.0000e+00 - val_loss: 1233.4681 - val_acc: 0.0000e+00\n",
      "Epoch 112/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 316.7061 - acc: 0.0000e+00 - val_loss: 1221.0860 - val_acc: 0.0000e+00\n",
      "Epoch 113/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.2822 - acc: 0.0000e+00 - val_loss: 1207.4875 - val_acc: 0.0000e+00\n",
      "Epoch 114/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.5002 - acc: 0.0000e+00 - val_loss: 1195.0510 - val_acc: 0.0000e+00\n",
      "Epoch 115/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 306.3132 - acc: 0.0000e+00 - val_loss: 1180.9348 - val_acc: 0.0000e+00\n",
      "Epoch 116/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 302.4815 - acc: 0.0625 - val_loss: 1168.3214 - val_acc: 0.0000e+00\n",
      "Epoch 117/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 299.0267 - acc: 0.0625 - val_loss: 1155.6747 - val_acc: 0.0000e+00\n",
      "Epoch 118/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 295.4111 - acc: 0.0625 - val_loss: 1143.8462 - val_acc: 0.0000e+00\n",
      "Epoch 119/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 292.0731 - acc: 0.0625 - val_loss: 1131.4494 - val_acc: 0.0000e+00\n",
      "Epoch 120/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 288.7624 - acc: 0.0625 - val_loss: 1118.5683 - val_acc: 0.0000e+00\n",
      "Epoch 121/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 285.1354 - acc: 0.0625 - val_loss: 1107.4212 - val_acc: 0.0000e+00\n",
      "Epoch 122/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.9478 - acc: 0.0625 - val_loss: 1095.0750 - val_acc: 0.0000e+00\n",
      "Epoch 123/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 278.6044 - acc: 0.0625 - val_loss: 1082.7928 - val_acc: 0.0000e+00\n",
      "Epoch 124/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.3881 - acc: 0.0625 - val_loss: 1070.2013 - val_acc: 0.0000e+00\n",
      "Epoch 125/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.8934 - acc: 0.0625 - val_loss: 1058.9391 - val_acc: 0.0000e+00\n",
      "Epoch 126/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 268.6903 - acc: 0.0625 - val_loss: 1047.3953 - val_acc: 0.0000e+00\n",
      "Epoch 127/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.6297 - acc: 0.0625 - val_loss: 1034.9728 - val_acc: 0.0000e+00\n",
      "Epoch 128/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 262.2524 - acc: 0.0625 - val_loss: 1023.7533 - val_acc: 0.0000e+00\n",
      "Epoch 129/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 259.2599 - acc: 0.0625 - val_loss: 1011.5411 - val_acc: 0.0000e+00\n",
      "Epoch 130/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 256.1111 - acc: 0.0625 - val_loss: 999.6186 - val_acc: 0.0000e+00\n",
      "Epoch 131/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 252.8058 - acc: 0.0625 - val_loss: 988.7567 - val_acc: 0.0000e+00\n",
      "Epoch 132/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 249.7693 - acc: 0.0625 - val_loss: 977.6966 - val_acc: 0.0000e+00\n",
      "Epoch 133/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 246.7024 - acc: 0.0625 - val_loss: 966.7347 - val_acc: 0.0000e+00\n",
      "Epoch 134/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 243.9995 - acc: 0.0625 - val_loss: 954.2620 - val_acc: 0.0000e+00\n",
      "Epoch 135/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 240.5986 - acc: 0.0625 - val_loss: 943.8051 - val_acc: 0.0000e+00\n",
      "Epoch 136/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.5786 - acc: 0.0625 - val_loss: 933.9985 - val_acc: 0.0000e+00\n",
      "Epoch 137/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 234.9399 - acc: 0.0625 - val_loss: 922.1083 - val_acc: 0.0000e+00\n",
      "Epoch 138/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 231.6964 - acc: 0.0625 - val_loss: 912.0885 - val_acc: 0.0000e+00\n",
      "Epoch 139/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 229.0811 - acc: 0.0625 - val_loss: 900.4921 - val_acc: 0.0000e+00\n",
      "Epoch 140/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 226.0169 - acc: 0.0625 - val_loss: 890.0763 - val_acc: 0.0000e+00\n",
      "Epoch 141/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 223.0631 - acc: 0.0625 - val_loss: 880.3363 - val_acc: 0.0000e+00\n",
      "Epoch 142/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 220.2967 - acc: 0.0625 - val_loss: 870.3616 - val_acc: 0.0000e+00\n",
      "Epoch 143/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 217.6892 - acc: 0.0625 - val_loss: 859.1623 - val_acc: 0.0000e+00\n",
      "Epoch 144/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 214.8565 - acc: 0.0625 - val_loss: 848.4073 - val_acc: 0.0000e+00\n",
      "Epoch 145/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 212.0658 - acc: 0.0625 - val_loss: 838.0371 - val_acc: 0.0000e+00\n",
      "Epoch 146/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 209.2084 - acc: 0.0625 - val_loss: 828.3933 - val_acc: 0.0000e+00\n",
      "Epoch 147/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 206.5961 - acc: 0.0625 - val_loss: 818.2746 - val_acc: 0.0000e+00\n",
      "Epoch 148/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 203.7346 - acc: 0.0625 - val_loss: 809.4659 - val_acc: 0.0000e+00\n",
      "Epoch 149/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 201.2280 - acc: 0.0625 - val_loss: 799.6536 - val_acc: 0.0000e+00\n",
      "Epoch 150/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 198.6066 - acc: 0.0625 - val_loss: 789.8688 - val_acc: 0.0000e+00\n",
      "Epoch 151/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 196.1551 - acc: 0.0625 - val_loss: 779.4312 - val_acc: 0.0000e+00\n",
      "Epoch 152/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 193.3029 - acc: 0.0625 - val_loss: 770.7897 - val_acc: 0.0000e+00\n",
      "Epoch 153/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 190.7611 - acc: 0.0625 - val_loss: 762.2273 - val_acc: 0.0000e+00\n",
      "Epoch 154/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 188.4986 - acc: 0.0625 - val_loss: 751.6843 - val_acc: 0.0000e+00\n",
      "Epoch 155/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 185.7341 - acc: 0.0625 - val_loss: 742.8296 - val_acc: 0.0000e+00\n",
      "Epoch 156/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 183.3908 - acc: 0.0625 - val_loss: 733.1194 - val_acc: 0.0000e+00\n",
      "Epoch 157/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 180.9078 - acc: 0.0625 - val_loss: 723.5838 - val_acc: 0.0000e+00\n",
      "Epoch 158/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 178.4244 - acc: 0.0625 - val_loss: 714.4959 - val_acc: 0.0000e+00\n",
      "Epoch 159/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 175.9936 - acc: 0.0625 - val_loss: 705.4527 - val_acc: 0.0000e+00\n",
      "Epoch 160/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 173.5344 - acc: 0.0625 - val_loss: 696.9052 - val_acc: 0.0000e+00\n",
      "Epoch 161/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 171.2446 - acc: 0.0625 - val_loss: 687.9739 - val_acc: 0.0000e+00\n",
      "Epoch 162/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 168.7316 - acc: 0.0625 - val_loss: 680.2472 - val_acc: 0.0000e+00\n",
      "Epoch 163/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 166.7152 - acc: 0.0625 - val_loss: 670.6080 - val_acc: 0.0000e+00\n",
      "Epoch 164/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 164.2597 - acc: 0.0625 - val_loss: 661.9762 - val_acc: 0.0000e+00\n",
      "Epoch 165/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 161.9455 - acc: 0.0625 - val_loss: 653.5988 - val_acc: 0.0000e+00\n",
      "Epoch 166/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 159.7220 - acc: 0.0625 - val_loss: 645.0540 - val_acc: 0.0000e+00\n",
      "Epoch 167/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 157.3815 - acc: 0.0625 - val_loss: 637.2311 - val_acc: 0.0000e+00\n",
      "Epoch 168/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 155.1763 - acc: 0.0000e+00 - val_loss: 629.4782 - val_acc: 0.0000e+00\n",
      "Epoch 169/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 153.1005 - acc: 0.0000e+00 - val_loss: 621.0097 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 150.9420 - acc: 0.0000e+00 - val_loss: 612.5659 - val_acc: 0.0000e+00\n",
      "Epoch 171/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 148.7044 - acc: 0.0000e+00 - val_loss: 604.7381 - val_acc: 0.0000e+00\n",
      "Epoch 172/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 146.7321 - acc: 0.0000e+00 - val_loss: 596.1581 - val_acc: 0.0000e+00\n",
      "Epoch 173/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 144.5228 - acc: 0.0000e+00 - val_loss: 588.2530 - val_acc: 0.0000e+00\n",
      "Epoch 174/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 142.4237 - acc: 0.0000e+00 - val_loss: 580.5035 - val_acc: 0.0000e+00\n",
      "Epoch 175/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 140.2992 - acc: 0.0000e+00 - val_loss: 573.1983 - val_acc: 0.0000e+00\n",
      "Epoch 176/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 138.3098 - acc: 0.0000e+00 - val_loss: 565.6300 - val_acc: 0.0000e+00\n",
      "Epoch 177/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 136.3613 - acc: 0.0000e+00 - val_loss: 557.8531 - val_acc: 0.0000e+00\n",
      "Epoch 178/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 134.2952 - acc: 0.0000e+00 - val_loss: 550.4713 - val_acc: 0.0000e+00\n",
      "Epoch 179/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 132.3870 - acc: 0.0000e+00 - val_loss: 542.7141 - val_acc: 0.0000e+00\n",
      "Epoch 180/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 130.2804 - acc: 0.0000e+00 - val_loss: 536.0030 - val_acc: 0.0000e+00\n",
      "Epoch 181/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 128.4331 - acc: 0.0000e+00 - val_loss: 528.8281 - val_acc: 0.0000e+00\n",
      "Epoch 182/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 126.7770 - acc: 0.0000e+00 - val_loss: 520.5038 - val_acc: 0.0000e+00\n",
      "Epoch 183/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 124.6297 - acc: 0.0000e+00 - val_loss: 513.3503 - val_acc: 0.0000e+00\n",
      "Epoch 184/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 122.6620 - acc: 0.0000e+00 - val_loss: 506.8598 - val_acc: 0.0000e+00\n",
      "Epoch 185/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 120.9682 - acc: 0.0000e+00 - val_loss: 499.5266 - val_acc: 0.0000e+00\n",
      "Epoch 186/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 118.9876 - acc: 0.0000e+00 - val_loss: 493.1137 - val_acc: 0.0000e+00\n",
      "Epoch 187/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 117.3061 - acc: 0.0625 - val_loss: 485.9594 - val_acc: 0.0000e+00\n",
      "Epoch 188/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 115.4418 - acc: 0.0625 - val_loss: 479.3278 - val_acc: 0.0000e+00\n",
      "Epoch 189/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 113.7626 - acc: 0.0625 - val_loss: 472.1916 - val_acc: 0.0000e+00\n",
      "Epoch 190/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 111.8359 - acc: 0.0625 - val_loss: 466.1638 - val_acc: 0.0000e+00\n",
      "Epoch 191/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 110.2564 - acc: 0.0625 - val_loss: 459.2438 - val_acc: 0.0000e+00\n",
      "Epoch 192/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 108.4934 - acc: 0.0625 - val_loss: 452.6488 - val_acc: 0.0000e+00\n",
      "Epoch 193/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 106.7941 - acc: 0.0625 - val_loss: 446.1437 - val_acc: 0.0000e+00\n",
      "Epoch 194/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 105.0886 - acc: 0.0625 - val_loss: 439.8745 - val_acc: 0.0000e+00\n",
      "Epoch 195/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 103.4134 - acc: 0.0625 - val_loss: 433.8744 - val_acc: 0.0000e+00\n",
      "Epoch 196/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 101.9596 - acc: 0.0625 - val_loss: 426.8614 - val_acc: 0.0000e+00\n",
      "Epoch 197/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 100.1087 - acc: 0.0625 - val_loss: 421.1662 - val_acc: 0.0000e+00\n",
      "Epoch 198/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 98.6742 - acc: 0.0625 - val_loss: 414.6497 - val_acc: 0.0000e+00\n",
      "Epoch 199/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 97.0223 - acc: 0.0625 - val_loss: 408.5610 - val_acc: 0.0000e+00\n",
      "Epoch 200/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 95.3458 - acc: 0.0625 - val_loss: 403.2463 - val_acc: 0.0000e+00\n",
      "Epoch 201/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 93.9506 - acc: 0.0625 - val_loss: 396.9994 - val_acc: 0.0000e+00\n",
      "Epoch 202/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 92.3787 - acc: 0.0625 - val_loss: 391.0947 - val_acc: 0.0000e+00\n",
      "Epoch 203/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 90.7938 - acc: 0.0625 - val_loss: 385.8935 - val_acc: 0.0000e+00\n",
      "Epoch 204/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 89.4503 - acc: 0.0625 - val_loss: 379.6752 - val_acc: 0.0000e+00\n",
      "Epoch 205/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 87.8413 - acc: 0.0625 - val_loss: 374.3654 - val_acc: 0.0000e+00\n",
      "Epoch 206/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 86.5334 - acc: 0.0625 - val_loss: 368.2070 - val_acc: 0.0000e+00\n",
      "Epoch 207/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 84.9685 - acc: 0.0625 - val_loss: 362.8070 - val_acc: 0.0000e+00\n",
      "Epoch 208/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 83.5149 - acc: 0.0625 - val_loss: 357.7236 - val_acc: 0.0000e+00\n",
      "Epoch 209/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 82.2416 - acc: 0.0625 - val_loss: 351.8581 - val_acc: 0.0000e+00\n",
      "Epoch 210/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 80.7311 - acc: 0.0625 - val_loss: 346.8155 - val_acc: 0.0000e+00\n",
      "Epoch 211/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 79.5036 - acc: 0.0625 - val_loss: 341.0507 - val_acc: 0.0000e+00\n",
      "Epoch 212/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 78.0129 - acc: 0.0625 - val_loss: 336.1318 - val_acc: 0.0000e+00\n",
      "Epoch 213/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 76.7361 - acc: 0.0625 - val_loss: 330.8661 - val_acc: 0.0000e+00\n",
      "Epoch 214/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 75.3287 - acc: 0.0625 - val_loss: 326.4035 - val_acc: 0.0000e+00\n",
      "Epoch 215/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 74.1873 - acc: 0.0625 - val_loss: 320.8120 - val_acc: 0.0000e+00\n",
      "Epoch 216/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 72.8230 - acc: 0.0625 - val_loss: 315.8145 - val_acc: 0.0000e+00\n",
      "Epoch 217/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 71.5536 - acc: 0.0625 - val_loss: 310.8183 - val_acc: 0.0000e+00\n",
      "Epoch 218/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 70.2868 - acc: 0.0625 - val_loss: 306.0091 - val_acc: 0.0000e+00\n",
      "Epoch 219/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 69.0464 - acc: 0.0625 - val_loss: 301.3311 - val_acc: 0.0000e+00\n",
      "Epoch 220/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 67.9317 - acc: 0.0625 - val_loss: 296.0337 - val_acc: 0.0000e+00\n",
      "Epoch 221/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 66.5638 - acc: 0.0625 - val_loss: 291.8765 - val_acc: 0.0000e+00\n",
      "Epoch 222/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 65.4923 - acc: 0.0625 - val_loss: 286.9135 - val_acc: 0.0000e+00\n",
      "Epoch 223/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 64.2496 - acc: 0.0625 - val_loss: 282.4837 - val_acc: 0.0000e+00\n",
      "Epoch 224/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 63.1519 - acc: 0.0625 - val_loss: 277.7187 - val_acc: 0.0000e+00\n",
      "Epoch 225/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 61.9391 - acc: 0.0625 - val_loss: 273.4935 - val_acc: 0.0000e+00\n",
      "Epoch 226/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 60.8179 - acc: 0.0625 - val_loss: 269.3727 - val_acc: 0.0000e+00\n",
      "Epoch 227/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 59.7900 - acc: 0.0625 - val_loss: 264.6621 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 58.7279 - acc: 0.0625 - val_loss: 259.8859 - val_acc: 0.0000e+00\n",
      "Epoch 229/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 57.5269 - acc: 0.0625 - val_loss: 255.8634 - val_acc: 0.0000e+00\n",
      "Epoch 230/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 56.5251 - acc: 0.0625 - val_loss: 251.4839 - val_acc: 0.0000e+00\n",
      "Epoch 231/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 55.4168 - acc: 0.0625 - val_loss: 247.5267 - val_acc: 0.0000e+00\n",
      "Epoch 232/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 54.4001 - acc: 0.0625 - val_loss: 243.5023 - val_acc: 0.0000e+00\n",
      "Epoch 233/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 53.3998 - acc: 0.0625 - val_loss: 239.3749 - val_acc: 0.0000e+00\n",
      "Epoch 234/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 52.4218 - acc: 0.0625 - val_loss: 235.1588 - val_acc: 0.0000e+00\n",
      "Epoch 235/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 51.3846 - acc: 0.0625 - val_loss: 231.2805 - val_acc: 0.0000e+00\n",
      "Epoch 236/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 50.4384 - acc: 0.0625 - val_loss: 227.2766 - val_acc: 0.0000e+00\n",
      "Epoch 237/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 49.4634 - acc: 0.0625 - val_loss: 223.3835 - val_acc: 0.0000e+00\n",
      "Epoch 238/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 48.5015 - acc: 0.0625 - val_loss: 219.6171 - val_acc: 0.0000e+00\n",
      "Epoch 239/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 47.6097 - acc: 0.0625 - val_loss: 215.6505 - val_acc: 0.0000e+00\n",
      "Epoch 240/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 46.6114 - acc: 0.0625 - val_loss: 212.2093 - val_acc: 0.0000e+00\n",
      "Epoch 241/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 45.7626 - acc: 0.0625 - val_loss: 208.4292 - val_acc: 0.0000e+00\n",
      "Epoch 242/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 44.8464 - acc: 0.0625 - val_loss: 204.8861 - val_acc: 0.0000e+00\n",
      "Epoch 243/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 43.9854 - acc: 0.0625 - val_loss: 201.2492 - val_acc: 0.0000e+00\n",
      "Epoch 244/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 43.1269 - acc: 0.0625 - val_loss: 197.6335 - val_acc: 0.0000e+00\n",
      "Epoch 245/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 42.2441 - acc: 0.0625 - val_loss: 194.2026 - val_acc: 0.0000e+00\n",
      "Epoch 246/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 41.3896 - acc: 0.0625 - val_loss: 190.9165 - val_acc: 0.0000e+00\n",
      "Epoch 247/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 40.5779 - acc: 0.0625 - val_loss: 187.5963 - val_acc: 0.0000e+00\n",
      "Epoch 248/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 39.7711 - acc: 0.0625 - val_loss: 184.2522 - val_acc: 0.0000e+00\n",
      "Epoch 249/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 38.9934 - acc: 0.0625 - val_loss: 180.8054 - val_acc: 0.0000e+00\n",
      "Epoch 250/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 38.1428 - acc: 0.0625 - val_loss: 177.8024 - val_acc: 0.0000e+00\n",
      "Epoch 251/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 37.3748 - acc: 0.0625 - val_loss: 174.8128 - val_acc: 0.0000e+00\n",
      "Epoch 252/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 36.6863 - acc: 0.0625 - val_loss: 171.3513 - val_acc: 0.0000e+00\n",
      "Epoch 253/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.8842 - acc: 0.0625 - val_loss: 168.2509 - val_acc: 0.0000e+00\n",
      "Epoch 254/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 35.1319 - acc: 0.0625 - val_loss: 165.3362 - val_acc: 0.0000e+00\n",
      "Epoch 255/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 34.4260 - acc: 0.0625 - val_loss: 162.3173 - val_acc: 0.0000e+00\n",
      "Epoch 256/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 33.7102 - acc: 0.0625 - val_loss: 159.3018 - val_acc: 0.0000e+00\n",
      "Epoch 257/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.9734 - acc: 0.0625 - val_loss: 156.5435 - val_acc: 0.0000e+00\n",
      "Epoch 258/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 32.3345 - acc: 0.0625 - val_loss: 153.4703 - val_acc: 0.0000e+00\n",
      "Epoch 259/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 31.6320 - acc: 0.0625 - val_loss: 150.6113 - val_acc: 0.0000e+00\n",
      "Epoch 260/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 30.9279 - acc: 0.0625 - val_loss: 148.0134 - val_acc: 0.0000e+00\n",
      "Epoch 261/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 30.3240 - acc: 0.0625 - val_loss: 145.1053 - val_acc: 0.0000e+00\n",
      "Epoch 262/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 29.6702 - acc: 0.1250 - val_loss: 142.2749 - val_acc: 0.0000e+00\n",
      "Epoch 263/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 28.9903 - acc: 0.1250 - val_loss: 139.7768 - val_acc: 0.0000e+00\n",
      "Epoch 264/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 28.4379 - acc: 0.1250 - val_loss: 136.8806 - val_acc: 0.0000e+00\n",
      "Epoch 265/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 27.7692 - acc: 0.1250 - val_loss: 134.3487 - val_acc: 0.0000e+00\n",
      "Epoch 266/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 27.1555 - acc: 0.1250 - val_loss: 131.9486 - val_acc: 0.0000e+00\n",
      "Epoch 267/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 26.5819 - acc: 0.1250 - val_loss: 129.4510 - val_acc: 0.0000e+00\n",
      "Epoch 268/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 25.9933 - acc: 0.1250 - val_loss: 127.0431 - val_acc: 0.0000e+00\n",
      "Epoch 269/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 25.4792 - acc: 0.1250 - val_loss: 124.3526 - val_acc: 0.0000e+00\n",
      "Epoch 270/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 24.8373 - acc: 0.0625 - val_loss: 122.2305 - val_acc: 0.0000e+00\n",
      "Epoch 271/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 24.3577 - acc: 0.0625 - val_loss: 119.6804 - val_acc: 0.0000e+00\n",
      "Epoch 272/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 23.8063 - acc: 0.0625 - val_loss: 117.2168 - val_acc: 0.0000e+00\n",
      "Epoch 273/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 23.2207 - acc: 0.0625 - val_loss: 115.1535 - val_acc: 0.0000e+00\n",
      "Epoch 274/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 22.7156 - acc: 0.0625 - val_loss: 113.0260 - val_acc: 0.0000e+00\n",
      "Epoch 275/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 22.2320 - acc: 0.0625 - val_loss: 110.7346 - val_acc: 0.0000e+00\n",
      "Epoch 276/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 21.7378 - acc: 0.0625 - val_loss: 108.4441 - val_acc: 0.0000e+00\n",
      "Epoch 277/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 21.2139 - acc: 0.0625 - val_loss: 106.4382 - val_acc: 0.0000e+00\n",
      "Epoch 278/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 20.7654 - acc: 0.0625 - val_loss: 104.2337 - val_acc: 0.0000e+00\n",
      "Epoch 279/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 20.2622 - acc: 0.0625 - val_loss: 102.2792 - val_acc: 0.0000e+00\n",
      "Epoch 280/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 19.8098 - acc: 0.0625 - val_loss: 100.2541 - val_acc: 0.0000e+00\n",
      "Epoch 281/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 19.3971 - acc: 0.0625 - val_loss: 98.0081 - val_acc: 0.0000e+00\n",
      "Epoch 282/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 18.8832 - acc: 0.0625 - val_loss: 96.3255 - val_acc: 0.0000e+00\n",
      "Epoch 283/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 18.5176 - acc: 0.0625 - val_loss: 94.1205 - val_acc: 0.0000e+00\n",
      "Epoch 284/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 18.0604 - acc: 0.0625 - val_loss: 92.1642 - val_acc: 0.0000e+00\n",
      "Epoch 285/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 17.6191 - acc: 0.0625 - val_loss: 90.3744 - val_acc: 0.0000e+00\n",
      "Epoch 286/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 17.2148 - acc: 0.1250 - val_loss: 88.5744 - val_acc: 0.0000e+00\n",
      "Epoch 287/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 16.8224 - acc: 0.1250 - val_loss: 86.7544 - val_acc: 0.0000e+00\n",
      "Epoch 288/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 16.4534 - acc: 0.1250 - val_loss: 84.8271 - val_acc: 0.0000e+00\n",
      "Epoch 289/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 16.0206 - acc: 0.1250 - val_loss: 83.2332 - val_acc: 0.0000e+00\n",
      "Epoch 290/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 15.6863 - acc: 0.1250 - val_loss: 81.3960 - val_acc: 0.0000e+00\n",
      "Epoch 291/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 15.2944 - acc: 0.1250 - val_loss: 79.7123 - val_acc: 0.0000e+00\n",
      "Epoch 292/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 14.9295 - acc: 0.1250 - val_loss: 78.0867 - val_acc: 0.0000e+00\n",
      "Epoch 293/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 14.5677 - acc: 0.1250 - val_loss: 76.5503 - val_acc: 0.0000e+00\n",
      "Epoch 294/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 14.2204 - acc: 0.1250 - val_loss: 75.0588 - val_acc: 0.0000e+00\n",
      "Epoch 295/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 13.8923 - acc: 0.1250 - val_loss: 73.5302 - val_acc: 0.0000e+00\n",
      "Epoch 296/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 13.6028 - acc: 0.1250 - val_loss: 71.7843 - val_acc: 0.0000e+00\n",
      "Epoch 297/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 13.2470 - acc: 0.1250 - val_loss: 70.2352 - val_acc: 0.0000e+00\n",
      "Epoch 298/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 12.9058 - acc: 0.1250 - val_loss: 68.8882 - val_acc: 0.0000e+00\n",
      "Epoch 299/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 12.6271 - acc: 0.1250 - val_loss: 67.3511 - val_acc: 0.0000e+00\n",
      "Epoch 300/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 12.2982 - acc: 0.0625 - val_loss: 66.0573 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# 하나의 은닉층을 사용하는 경우\n",
    "model = Sequential()\n",
    "print(type(model)) ## Class 첫자 대문자 Dense\n",
    "# 입력값 : input_shape\n",
    "# 1 출력갯수\n",
    "# 활성화 함수 : linear\n",
    "#model.add(Dense(1,input_shape=(1,), activation='linear')) # 배열 선언\n",
    "model.add(Dense( 1, input_dim=1, activation='linear')) # 입력갯수\n",
    "model.compile( optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "# 학습 validation_split=0.2 검증과정에서  사용(4건)\n",
    "# 전체데이터를 대상으로 한 학습지수 epochs\n",
    "# batch_size : 전체데이터를 사용하면 메모리 부족현상, 데이터를 분할하여 학습하는 기법, 가중치 변경\n",
    "# 총 가중치 변경 : epochs * batch_size : 1*300=300\n",
    "hist = model.fit( x_train, y_train,validation_split=0.2, epochs=300, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# 파라메터\n",
    "# 하이퍼 파라메터 : 학습에 사용되는 파라메터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "font_name = font_manager.FontProperties(fname=\"C:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "# windows 10\n",
    "# font_name = font_manager.FontProperties(fname=\"C:/Windows/Fonts/malgunsl.ttf\").get_name()\n",
    "rc('font', family=font_name)           # 맑은 고딕 폰트 지정\n",
    "plt.rcParams[\"font.size\"] = 12         # 글자 크기\n",
    "# plt.rcParams[\"figure.figsize\"] = (10, 4) # 10:4의 그래프 비율\n",
    "plt.rcParams['axes.unicode_minus'] = False  # minus 부호는 unicode 적용시 한글이 깨짐으로 설정\n",
    "\n",
    "# Jupyter에게 matplotlib 그래프를 출력 영역에 표시할 것을 지시하는 명령\n",
    "%matplotlib inline  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "fig.set_size_inches(10, 5)  # 챠트 크기 설정\n",
    " \n",
    "acc_ax = loss_ax.twinx()   # x 축을 공유하는 새로운 Axes 객체를 만든다.\n",
    " \n",
    "loss_ax.set_ylim([0.0, 1.0])\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    " \n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    " \n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    " \n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.sequential.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "# 여러 은닉층을 사용하는 경우\n",
    "model = Sequential()\n",
    "print(type(model)) ## Class 첫자 대문자 Dense\n",
    "# 입력값 : input_shape\n",
    "# 10 출력갯수\n",
    "# 활성화 함수 : linear\n",
    "#model.add(Dense(10,input_shape=(1,), activation='linear')) # 배열 선언\n",
    "model.add(Dense(10,input_dim=1, activation='linear')) # 입력갯수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine",
   "language": "python",
   "name": "machine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
